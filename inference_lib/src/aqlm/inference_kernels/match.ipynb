{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aqlm import QuantizedLinear\n",
    "from aqlm.utils import _dequantize_weight, unpack_int_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Using python executable 'python'\n",
      "-- Resolved buck2 as /Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/buck2-bin/buck2-99773fe6f7963a72ae5f7b737c02836e.\n",
      "-- Killing buck2 daemon\n",
      "-- executorch: Generating source lists\n",
      "-- executorch: Using source file list /Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/executorch_srcs.cmake\n",
      "-- executorch: Using sources file /Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/executorch_srcs.cmake\n",
      "-- Proceeding with version: 24.3.25.0\n",
      "-- CMAKE_CXX_FLAGS: \n",
      "-- Generating operator lib:\n",
      "--   LIB_NAME: portable_ops_lib\n",
      "--   OPS_SCHEMA_YAML: /Users/blacksamorez/reps/executorch/kernels/portable/functions.yaml\n",
      "--   ROOT_OPS: \n",
      "--   INCLUDE_ALL_OPS: \n",
      "\u001b[0mCommand - python;-m;codegen.tools.gen_oplist;--output_path=/Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/kernels/portable/portable_ops_lib/selected_operators.yaml;--ops_schema_yaml_path=\"/Users/blacksamorez/reps/executorch/kernels/portable/functions.yaml\"\u001b[0m\n",
      "-- Generating kernel bindings:\n",
      "--   LIB_NAME: portable_ops_lib\n",
      "--   FUNCTIONS_YAML: /Users/blacksamorez/reps/executorch/kernels/portable/functions.yaml\n",
      "--   CUSTOM_OPS_YAML: \n",
      "\u001b[0mGenerated files /Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/kernels/portable/portable_ops_lib/RegisterCodegenUnboxedKernelsEverything.cpp;/Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/kernels/portable/portable_ops_lib/Functions.h;/Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/kernels/portable/portable_ops_lib/NativeFunctions.h\u001b[0m\n",
      "-- Generating operator lib:\n",
      "--   LIB_NAME: portable_ops_lib\n",
      "--   KERNEL_LIBS: portable_kernels\n",
      "--   DEPS: executorch\n",
      "-- Generating operator lib:\n",
      "--   LIB_NAME: optimized_ops_lib\n",
      "--   OPS_SCHEMA_YAML: /Users/blacksamorez/reps/executorch/kernels/optimized/optimized-oss.yaml\n",
      "--   ROOT_OPS: \n",
      "--   INCLUDE_ALL_OPS: \n",
      "\u001b[0mCommand - python;-m;codegen.tools.gen_oplist;--output_path=/Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/kernels/optimized/optimized_ops_lib/selected_operators.yaml;--ops_schema_yaml_path=\"/Users/blacksamorez/reps/executorch/kernels/optimized/optimized-oss.yaml\"\u001b[0m\n",
      "-- Generating kernel bindings:\n",
      "--   LIB_NAME: optimized_ops_lib\n",
      "--   FUNCTIONS_YAML: /Users/blacksamorez/reps/executorch/kernels/optimized/optimized-oss.yaml\n",
      "--   CUSTOM_OPS_YAML: \n",
      "\u001b[0mGenerated files /Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/kernels/optimized/optimized_ops_lib/RegisterCodegenUnboxedKernelsEverything.cpp;/Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/kernels/optimized/optimized_ops_lib/Functions.h;/Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/kernels/optimized/optimized_ops_lib/NativeFunctions.h\u001b[0m\n",
      "-- Generating operator lib:\n",
      "--   LIB_NAME: optimized_ops_lib\n",
      "--   KERNEL_LIBS: optimized_kernels\n",
      "--   DEPS: executorch\n",
      "-- Merging kernel yaml files:\n",
      "--   FUNCTIONS_YAML: /Users/blacksamorez/reps/executorch/configurations/../kernels/optimized/optimized-oss.yaml\n",
      "--   FALLBACK_YAML: /Users/blacksamorez/reps/executorch/configurations/../kernels/portable/functions.yaml\n",
      "--   OUTPUT_DIR: /Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/configurations\n",
      "-- Generating operator lib:\n",
      "--   LIB_NAME: optimized_native_cpu_ops_lib\n",
      "--   OPS_SCHEMA_YAML: /Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/configurations/merged.yaml\n",
      "--   ROOT_OPS: \n",
      "--   INCLUDE_ALL_OPS: \n",
      "\u001b[0mCommand - python;-m;codegen.tools.gen_oplist;--output_path=/Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/configurations/optimized_native_cpu_ops_lib/selected_operators.yaml;--ops_schema_yaml_path=\"/Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/configurations/merged.yaml\"\u001b[0m\n",
      "-- Generating kernel bindings:\n",
      "--   LIB_NAME: optimized_native_cpu_ops_lib\n",
      "--   FUNCTIONS_YAML: /Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/configurations/merged.yaml\n",
      "--   CUSTOM_OPS_YAML: \n",
      "\u001b[0mGenerated files /Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/configurations/optimized_native_cpu_ops_lib/RegisterCodegenUnboxedKernelsEverything.cpp;/Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/configurations/optimized_native_cpu_ops_lib/Functions.h;/Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/configurations/optimized_native_cpu_ops_lib/NativeFunctions.h\u001b[0m\n",
      "-- Generating operator lib:\n",
      "--   LIB_NAME: optimized_native_cpu_ops_lib\n",
      "--   KERNEL_LIBS: portable_kernels;optimized_kernels\n",
      "--   DEPS: executorch\n",
      "\u001b[0mCMake Deprecation Warning at /Users/blacksamorez/reps/executorch/third-party/gflags/CMakeLists.txt:73 (cmake_minimum_required):\n",
      "  Compatibility with CMake < 3.5 will be removed from a future version of\n",
      "  CMake.\n",
      "\n",
      "  Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
      "  CMake that the project does not need compatibility with older versions.\n",
      "\n",
      "\u001b[0m\n",
      "-- \n",
      "-- ******** Summary ********\n",
      "--   CMAKE_BUILD_TYPE              : Debug\n",
      "--   CMAKE_CXX_STANDARD            : 17\n",
      "--   CMAKE_CXX_COMPILER_ID         : AppleClang\n",
      "--   CMAKE_TOOLCHAIN_FILE          : \n",
      "--   BUCK2                         : /Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/buck2-bin/buck2-99773fe6f7963a72ae5f7b737c02836e\n",
      "--   PYTHON_EXECUTABLE             : python\n",
      "--   FLATC_EXECUTABLE              : flatc\n",
      "--   EXECUTORCH_ENABLE_LOGGING              : ON\n",
      "--   EXECUTORCH_ENABLE_PROGRAM_VERIFICATION : ON\n",
      "--   EXECUTORCH_LOG_LEVEL                   : Info\n",
      "--   EXECUTORCH_BUILD_ANDROID_JNI           : OFF\n",
      "--   EXECUTORCH_BUILD_ARM_BAREMETAL         : OFF\n",
      "--   EXECUTORCH_BUILD_COREML                : OFF\n",
      "--   EXECUTORCH_BUILD_KERNELS_CUSTOM        : OFF\n",
      "--   EXECUTORCH_BUILD_EXECUTOR_RUNNER       : ON\n",
      "--   EXECUTORCH_BUILD_EXTENSION_DATA_LOADER : OFF\n",
      "--   EXECUTORCH_BUILD_EXTENSION_MODULE      : OFF\n",
      "--   EXECUTORCH_BUILD_EXTENSION_RUNNER_UTIL : OFF\n",
      "--   EXECUTORCH_BUILD_FLATC                 : ON\n",
      "--   EXECUTORCH_BUILD_GFLAGS                : ON\n",
      "--   EXECUTORCH_BUILD_GTESTS                : OFF\n",
      "--   EXECUTORCH_BUILD_HOST_TARGETS          : ON\n",
      "--   EXECUTORCH_BUILD_MPS                   : OFF\n",
      "--   EXECUTORCH_BUILD_PYBIND                : OFF\n",
      "--   EXECUTORCH_BUILD_QNN                   : OFF\n",
      "--   EXECUTORCH_BUILD_KERNELS_OPTIMIZED     : ON\n",
      "--   EXECUTORCH_BUILD_KERNELS_QUANTIZED     : OFF\n",
      "--   EXECUTORCH_BUILD_SDK                   : OFF\n",
      "--   EXECUTORCH_BUILD_SIZE_TEST             : OFF\n",
      "--   EXECUTORCH_BUILD_XNNPACK               : OFF\n",
      "--   EXECUTORCH_BUILD_VULKAN                : OFF\n",
      "--   EXECUTORCH_BUILD_PTHREADPOOL           : ON\n",
      "--   EXECUTORCH_BUILD_CPUINFO               : ON\n",
      "-- Configuring done (0.3s)\n",
      "-- Generating done (0.1s)\n",
      "-- Build files have been written to: /Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/cmake-out\n",
      "[  2%] Built target pthreadpool\n",
      "[  8%] Built target cpuinfo\n",
      "[  8%] Built target cpuinfo_internals\n",
      "[ 10%] Built target gflags_nothreads_static\n",
      "[ 17%] Built target eigen_blas\n",
      "[ 29%] Built target flatc\n",
      "[ 29%] \u001b[34m\u001b[1mGenerating common_schema headers\u001b[0m\n",
      "[ 29%] Built target common_schema\n",
      "[ 29%] \u001b[34m\u001b[1mGenerating program_schema headers\u001b[0m\n",
      "[ 29%] Built target program_schema\n",
      "[ 29%] \u001b[32mBuilding CXX object /Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/CMakeFiles/executorch_no_prim_ops.dir/runtime/executor/method.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object /Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/CMakeFiles/executorch_no_prim_ops.dir/runtime/executor/program.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object /Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/CMakeFiles/executorch_no_prim_ops.dir/runtime/executor/method_meta.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object /Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/CMakeFiles/executorch_no_prim_ops.dir/runtime/executor/tensor_parser_exec_aten.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object /Users/blacksamorez/reps/AQLM/inference_lib/src/aqlm/inference_kernels/executorch/CMakeFiles/executorch_no_prim_ops.dir/runtime/executor/tensor_parser_portable.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32m\u001b[1mLinking CXX static library libexecutorch_no_prim_ops.a\u001b[0m\n",
      "[ 34%] Built target executorch_no_prim_ops\n",
      "[ 35%] Built target cpublas\n",
      "[ 37%] Built target executorch\n",
      "[ 37%] \u001b[32m\u001b[1mLinking CXX shared library libaqlm.dylib\u001b[0m\n",
      "[ 40%] Built target optimized_kernels\n",
      "[ 41%] Built target optimized_ops_lib\n",
      "[ 41%] Built target aqlm\n",
      "[ 42%] \u001b[32m\u001b[1mLinking CXX shared library libaqlm_bindings.dylib\u001b[0m\n",
      "[ 42%] Built target aqlm_bindings\n",
      "[ 94%] Built target portable_kernels\n",
      "[ 98%] Built target portable_ops_lib\n",
      "[ 98%] Built target optimized_native_cpu_ops_lib\n",
      "[ 98%] \u001b[32m\u001b[1mLinking CXX executable executor_runner\u001b[0m\n",
      "ld: warning: ignoring duplicate libraries: 'libexecutorch.a'\n",
      "[100%] Built target executor_runner\n"
     ]
    }
   ],
   "source": [
    "!bash build.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 1024\n",
    "\n",
    "layer = QuantizedLinear(\n",
    "    in_features=SIZE,\n",
    "    out_features=SIZE * 3,\n",
    "    in_group_size=8,\n",
    "    out_group_size=1,\n",
    "    num_codebooks=2,\n",
    "    nbits_per_codebook=8,\n",
    "    bias=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_weight = _dequantize_weight(\n",
    "    unpack_int_data(layer.codes, 8),\n",
    "    layer.codebooks,\n",
    "    layer.scales,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input = torch.rand((3, 2, SIZE)) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.ops.load_library(\"./cmake-out/libaqlm_bindings.dylib\")\n",
    "\n",
    "reference = input @ reference_weight.T + layer.bias\n",
    "test = torch.ops.aqlm.code2x8_lut_matmat(\n",
    "    input,\n",
    "    torch.permute(layer.codes, (1, 0, 2)).contiguous(),\n",
    "    layer.codebooks,\n",
    "    layer.scales,\n",
    "    layer.bias,\n",
    ")\n",
    "\n",
    "torch.testing.assert_close(\n",
    "    test,\n",
    "    reference,\n",
    "    atol=0.01,\n",
    "    rtol=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.6 ms, sys: 17 ms, total: 38.6 ms\n",
      "Wall time: 10.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(10):\n",
    "    input @ reference_weight.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 857 ms, sys: 239 ms, total: 1.1 s\n",
      "Wall time: 201 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(10):\n",
    "    torch.ops.aqlm.code2x8_lut_matmat(\n",
    "        input,\n",
    "        torch.permute(layer.codes, (1, 0, 2)).contiguous(),\n",
    "        layer.codebooks,\n",
    "        layer.scales,\n",
    "        layer.bias,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/executorch/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:362: UserWarning: At pre-dispatch tracing, we will assume that any custom op that is marked with CompositeImplicitAutograd and functional are safe to not decompose. We found aqlm.code2x8_lut_matmat.default to be one such op.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.export import export, ExportedProgram, Dim\n",
    "from executorch.exir import EdgeCompileConfig, to_edge\n",
    "\n",
    "_ = layer(input)\n",
    "\n",
    "batch_size = Dim(\"batch_size\", min=1)\n",
    "seq_len = Dim(\"seq_len\", min=1)\n",
    "dynamic_shapes = {\"input\": {0: batch_size, 1: seq_len}}\n",
    "\n",
    "with torch.no_grad():\n",
    "    aten_dialect = export(layer, (input,), dynamic_shapes=dynamic_shapes)\n",
    "    \n",
    "edge_manager = to_edge(aten_dialect, compile_config=EdgeCompileConfig(_check_ir_validity=False))\n",
    "et_program = edge_manager.to_executorch()\n",
    "\n",
    "with open(\"aqlm.pte\", \"wb\") as file:\n",
    "    file.write(et_program.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
