{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aqlm import QuantizedLinear\n",
    "from aqlm.utils import _dequantize_weight, unpack_int_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd /Users/blacksamorez/reps/executorch && bash build.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 1024\n",
    "\n",
    "layer = QuantizedLinear(\n",
    "    in_features=SIZE,\n",
    "    out_features=SIZE * 3,\n",
    "    in_group_size=8,\n",
    "    out_group_size=1,\n",
    "    num_codebooks=2,\n",
    "    nbits_per_codebook=8,\n",
    "    bias=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_weight = _dequantize_weight(\n",
    "    unpack_int_data(layer.codes, 8),\n",
    "    layer.codebooks,\n",
    "    layer.scales,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input = torch.rand((3, 2, SIZE)) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.ops.load_library(\"./cmake-out/libaqlm_bindings.dylib\")\n",
    "\n",
    "reference = input @ reference_weight.T + (layer.bias if layer.bias is not None else 0)\n",
    "test = torch.ops.aqlm.code2x8_lut_matmat(\n",
    "    input,\n",
    "    torch.permute(layer.codes, (1, 0, 2)).contiguous(),\n",
    "    layer.codebooks,\n",
    "    layer.scales,\n",
    "    bias=layer.bias,\n",
    ")\n",
    "\n",
    "torch.testing.assert_close(\n",
    "    test,\n",
    "    reference,\n",
    "    atol=0.01,\n",
    "    rtol=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.7 ms, sys: 11.9 ms, total: 34.6 ms\n",
      "Wall time: 6.31 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(10):\n",
    "    input @ reference_weight.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 996 ms, sys: 127 ms, total: 1.12 s\n",
      "Wall time: 193 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(10):\n",
    "    torch.ops.aqlm.code2x8_lut_matmat(\n",
    "        input,\n",
    "        torch.permute(layer.codes, (1, 0, 2)).contiguous(),\n",
    "        layer.codebooks,\n",
    "        layer.scales,\n",
    "        bias=layer.bias,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/executorch/lib/python3.10/site-packages/executorch/exir/passes/_quant_patterns_and_replacements.py:106: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @impl_abstract(\"quantized_decomposed::embedding_byte.out\")\n",
      "/opt/miniconda3/envs/executorch/lib/python3.10/site-packages/executorch/exir/passes/_quant_patterns_and_replacements.py:153: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @impl_abstract(\"quantized_decomposed::embedding_byte.dtype_out\")\n",
      "/opt/miniconda3/envs/executorch/lib/python3.10/site-packages/executorch/exir/passes/_quant_patterns_and_replacements.py:228: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @impl_abstract(\"quantized_decomposed::embedding_4bit.out\")\n",
      "/opt/miniconda3/envs/executorch/lib/python3.10/site-packages/executorch/exir/passes/_quant_patterns_and_replacements.py:281: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @impl_abstract(\"quantized_decomposed::embedding_4bit.dtype_out\")\n",
      "/opt/miniconda3/envs/executorch/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:364: UserWarning: At pre-dispatch tracing, we assume that any custom op marked with CompositeImplicitAutograd and have functional schema are safe to not decompose. Found aqlm.code2x8_lut_matmat.default to be one such op.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/executorch/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:364: UserWarning: At pre-dispatch tracing, we assume that any custom op marked with CompositeImplicitAutograd and have functional schema are safe to not decompose. Found aqlm.code2x8_lut_matmat.default to be one such op.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.export import export, ExportedProgram, Dim\n",
    "from executorch.exir import EdgeCompileConfig, to_edge\n",
    "\n",
    "_ = layer(input)\n",
    "\n",
    "batch_size = Dim(\"batch_size\", min=1)\n",
    "seq_len = Dim(\"seq_len\", min=1)\n",
    "dynamic_shapes = {\"input\": {0: batch_size, 1: seq_len}}\n",
    "\n",
    "with torch.no_grad():\n",
    "    aten_dialect = export(layer, (input,), dynamic_shapes=dynamic_shapes)\n",
    "    \n",
    "edge_manager = to_edge(aten_dialect, compile_config=EdgeCompileConfig(_check_ir_validity=False))\n",
    "\n",
    "et_program = edge_manager.to_executorch()\n",
    "\n",
    "with open(\"aqlm.pte\", \"wb\") as file:\n",
    "    file.write(et_program.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transposing codes layers.0.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.0.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.0.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.0.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.1.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.1.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.1.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.1.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.10.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.10.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.10.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.10.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.11.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.11.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.11.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.11.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.12.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.12.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.12.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.12.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.13.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.13.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.13.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.13.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.14.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.14.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.14.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.14.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.15.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.15.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.15.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.15.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.16.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.16.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.16.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.16.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.17.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.17.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.17.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.17.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.18.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.18.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.18.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.18.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.19.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.19.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.19.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.19.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.2.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.2.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.2.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.2.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.20.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.20.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.20.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.20.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.21.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.21.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.21.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.21.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.22.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.22.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.22.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.22.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.23.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.23.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.23.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.23.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.24.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.24.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.24.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.24.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.25.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.25.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.25.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.25.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.26.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.26.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.26.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.26.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.27.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.27.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.27.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.27.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.28.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.28.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.28.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.28.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.29.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.29.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.29.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.29.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.3.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.3.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.3.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.3.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.30.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.30.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.30.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.30.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.31.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.31.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.31.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.31.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.4.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.4.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.4.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.4.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.5.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.5.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.5.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.5.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.6.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.6.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.6.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.6.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.7.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.7.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.7.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.7.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.8.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.8.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.8.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.8.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.9.attention.wk.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.9.attention.wk.scales value.shape=torch.Size([4096, 1, 1, 1])\n",
      "Transposing codes layers.9.attention.wq.codes value.shape=torch.Size([4096, 512, 2])\n",
      "Transposing scales layers.9.attention.wq.scales value.shape=torch.Size([4096, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "dict = load_file(\"/Users/blacksamorez/models/Llama-2-7b-AQLM-2Bit-2x8-hf/model.safetensors\")\n",
    "\n",
    "mapping = {\n",
    "    \"model.\": \"\",\n",
    "    \n",
    "    \"self_attn.q_proj\": \"attention.wq\",\n",
    "    \"self_attn.k_proj\": \"attention.wk\",\n",
    "    \"self_attn.v_proj\": \"attention.wv\",\n",
    "    \"self_attn.o_proj\": \"attention.wo\",\n",
    "    \n",
    "    \"mlp.up_proj\": \"feed_forward.w3\",\n",
    "    \"mlp.gate_proj\": \"feed_forward.w1\",\n",
    "    \"mlp.down_proj\": \"feed_forward.w2\",\n",
    "    \n",
    "    \"input_layernorm\": \"attention_norm\",\n",
    "    \"post_attention_layernorm\": \"ffn_norm\",\n",
    "    \n",
    "    \"lm_head\": \"output\",\n",
    "    \"embed_tokens\": \"tok_embeddings\",\n",
    "}\n",
    "\n",
    "\n",
    "new_dict = {}\n",
    "\n",
    "for key, value in dict.items():\n",
    "    for old, new in mapping.items():\n",
    "        key = key.replace(old, new)\n",
    "        \n",
    "    if \"attention.wq.codes\" in key or \"attention.wk.codes\" in key:\n",
    "        # [num_out_groups, num_in_groups, num_codebooks]\n",
    "        print(f\"Transposing codes {key} {value.shape=}\")\n",
    "        value = (value.reshape(32, 2, 128 // 2, -1, 2)\n",
    "            .transpose(1, 2)\n",
    "            .reshape(128 * 32, -1, 2))\n",
    "        \n",
    "    if \"attention.wq.scales\" in key or \"attention.wk.scales\" in key:\n",
    "        # [num_out_groups, 1, 1, 1]\n",
    "        print(f\"Transposing scales {key} {value.shape=}\")\n",
    "        value = (value.reshape(32, 2, 128 // 2, 1)\n",
    "            .transpose(1, 2)\n",
    "            .reshape(128 * 32, 1, 1, 1))\n",
    "    \n",
    "    new_dict[key] = value\n",
    "    \n",
    "# del new_dict[\"output.weight\"]\n",
    "# del new_dict[\"tok_embeddings.weight\"]\n",
    "\n",
    "torch.save(new_dict, \"/Users/blacksamorez/models/Llama-2-7b-AQLM-2Bit-2x8-hf/executorch.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
