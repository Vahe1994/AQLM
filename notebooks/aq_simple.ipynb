{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43871e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: TRANSFORMERS_CACHE=/mnt/LLM/hub\n",
      "env: HF_HOME=/mnt/LLM/hub\n",
      "env: OMP_NUM_THREADS=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jheuristic/anaconda3/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env TRANSFORMERS_CACHE=/mnt/LLM/hub\n",
    "%env HF_HOME=/mnt/LLM/hub\n",
    "%env OMP_NUM_THREADS=16\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import time\n",
    "import random\n",
    "from tqdm.auto import trange\n",
    "import ipynbname  # pip install ipynbname\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "\n",
    "from src.aq import QuantizedWeight\n",
    "\n",
    "\n",
    "torch.set_num_threads(16)\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "input_loading_dir = '/extra_disk_1/vahe1994/BRRR/layer10.self_attn.q_proj.input_activation.pt'  # <-- stealing from Vahe\n",
    "num_codebooks = 4\n",
    "nbits_per_codebook = 8\n",
    "out_group_size = 1\n",
    "in_group_size = 16\n",
    "batch_size = 16384\n",
    "beam_size = 1\n",
    "beam_search_epochs = 100\n",
    "print_frequency = 10\n",
    "scale_nbits = 0    # 0 means no scales, 16 means no compression;\n",
    "codebook_values_nbits = 16  # less than 16 means we quantize codebooks as well\n",
    "init_max_iter = 100\n",
    "entropy_regularizer = 1e-3\n",
    "entropy_warmup_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7e6156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjustheuristic\u001b[0m (\u001b[33mrock-and-roll\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jheuristic/AQLM_entropy/notebooks/wandb/run-20240228_203713-ufxs2qwf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rock-and-roll/AddQuantization/runs/ufxs2qwf' target=\"_blank\">aq_simple_AQ_num_codebooks=4_out_group_size=1_in_group_size=16_nbits_per_codebook=8_beam_search_epochs=100</a></strong> to <a href='https://wandb.ai/rock-and-roll/AddQuantization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rock-and-roll/AddQuantization' target=\"_blank\">https://wandb.ai/rock-and-roll/AddQuantization</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rock-and-roll/AddQuantization/runs/ufxs2qwf' target=\"_blank\">https://wandb.ai/rock-and-roll/AddQuantization/runs/ufxs2qwf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No relevant files were detected in the specified directory. No code will be logged to your run.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = os.path.join(os.getcwd(), ipynbname.name() + \".ipynb\")\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    dir=os.getcwd(),\n",
    "    project=\"AddQuantization\",\n",
    "    entity = \"rock-and-roll\",\n",
    "    save_code=True,\n",
    "    name = f\"{ipynbname.name()}_AQ_{num_codebooks=}_{out_group_size=}_{in_group_size=}_{nbits_per_codebook=}_{beam_search_epochs=}\",\n",
    "    settings=wandb.Settings(code_dir=\".\"),\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"num_codebooks\" : num_codebooks,\n",
    "    \"out_group_size\": out_group_size,\n",
    "    \"in_group_size\": in_group_size,\n",
    "    \"group_size\" : out_group_size * in_group_size,\n",
    "    \"batch_size\" : batch_size,\n",
    "    \"beam_size\" : beam_size,\n",
    "    \"nbits_per_codebook\" : nbits_per_codebook,\n",
    "    \"codebook_values_nbits\": codebook_values_nbits,\n",
    "    \"scale_nbits\": scale_nbits,\n",
    "    \"beam_search_epochs\": beam_search_epochs,\n",
    "    \"init_max_iter\": init_max_iter,\n",
    "    \"entropy_regularizer\": entropy_regularizer,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf7e1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29147dd099ce4de5b0f24de854a65caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f776ed6a4f64e4988dc0287253d02e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-70b-hf\", torch_dtype='auto', low_cpu_mem_usage=True)\n",
    "\n",
    "X = torch.load(input_loading_dir, map_location='cpu').float().flatten(0, -2)\n",
    "reference_weight = model.model.layers[10].self_attn.q_proj.weight.detach().to(device).float()\n",
    "\n",
    "XTX = torch.zeros(X.shape[-1], X.shape[-1], device=device, dtype=torch.float64)\n",
    "for i in range(0, len(X), batch_size):\n",
    "    x_batch = X[i: i + batch_size].cuda().double()\n",
    "    XTX.addmm_(x_batch.T, x_batch, alpha=1/len(X))\n",
    "    del x_batch\n",
    "XTX = XTX.float()\n",
    "del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e80497e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_code_frequencies(codes: torch.LongTensor):\n",
    "    code_counts = torch.zeros(num_codebooks, 2**nbits_per_codebook, dtype=torch.int64, device=codes.device)\n",
    "    for codebook_index in range(num_codebooks):\n",
    "        code_counts[codebook_index, :] = torch.bincount(\n",
    "            codes[..., codebook_index].flatten(), minlength=2**nbits_per_codebook)\n",
    "    return code_counts.float() / code_counts.sum(-1, keepdim=True)\n",
    "\n",
    "def _calculate_code_entropy(codes: torch.LongTensor):\n",
    "    \"\"\"Calculate per-codebook code entropy measured in bits (base-2)\"\"\"\n",
    "    probs = _calculate_code_frequencies(codes)\n",
    "    logprobs = torch.log2(probs.clamp_min(1e-12))\n",
    "    return - torch.sum(probs * logprobs, dim=-1)\n",
    "\n",
    "def _get_entropy_penalties_upper_bound(codes: torch.LongTensor, regularizer: float):\n",
    "    \"\"\"Compute log-probability penalties that minimize a linearized upper bound on entropy \"\"\"\n",
    "    probs = _calculate_code_frequencies(codes)\n",
    "    logprobs = torch.log2(probs.clamp_min(1e-12))\n",
    "    return (- regularizer / logprobs.shape[-1]) * logprobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ea22d6",
   "metadata": {},
   "source": [
    "# debugging code: run regularizer; compare entropy before/after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab41070b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4da1d75866499586c19375d66709a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "initializing with kmeans:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jheuristic/AQLM_entropy/notebooks/../src/aq.py:681: UserWarning: index_reduce() is in beta and the API may change at any time. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392022560/work/aten/src/ATen/native/cuda/Indexing.cu:1193.)\n",
      "  codebook_i, _, _ = fit_kmeans(\n"
     ]
    }
   ],
   "source": [
    "quantized_weight = QuantizedWeight(\n",
    "    XTX=XTX, reference_weight=reference_weight, num_codebooks=num_codebooks,\n",
    "    nbits_per_codebook=nbits_per_codebook, scale_nbits=scale_nbits, \n",
    "    out_group_size=out_group_size, in_group_size=in_group_size,\n",
    "    verbose=True, max_iter=init_max_iter,   # faster init, not tested\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfce3c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy before: tensor([7.9989, 7.9997, 7.9943, 7.9942], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy before:\", _calculate_code_entropy(quantized_weight.codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17b0b43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c234cdcbf0fa49adae83e8d5ff5b6a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy after: tensor([7.9732, 7.8986, 7.7041, 7.2989], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "code_penalties = _get_entropy_penalties_upper_bound(quantized_weight.codes, regularizer=0.01)\n",
    "quantized_weight.beam_search_update_codes_(\n",
    "    XTX, reference_weight, beam_size=beam_size, code_penalties=code_penalties,\n",
    "    dim_rng=random.Random(), verbose=True)\n",
    "print(\"Entropy after:\", _calculate_code_entropy(quantized_weight.codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1e99f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30912ccc1ca844a482ba77493e5f06a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy after: tensor([5.2803, 2.5830, 2.0586, 0.4563], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "code_penalties = _get_entropy_penalties_upper_bound(quantized_weight.codes, regularizer=0.01)\n",
    "quantized_weight.beam_search_update_codes_(\n",
    "    XTX, reference_weight, beam_size=beam_size, code_penalties=code_penalties,\n",
    "    dim_rng=random.Random(), verbose=True)\n",
    "print(\"Entropy after:\", _calculate_code_entropy(quantized_weight.codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ecd56ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE + sum_c  num_codes_equal_to(c) * code_penalty[c]\n",
    "\n",
    "# let code_penalty[j]  = - log code_probs[j]\n",
    "\n",
    "# MSE - sum_c  num_codes_equal_to(c) * ( log code_probs[c])\n",
    "\n",
    "# MSE + const * entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f555901a",
   "metadata": {},
   "source": [
    "# Main calibration code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8656f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe8f2582ec54cdca505a76bb00c7580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "initializing with kmeans:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG bits: 2.005859375\n",
      "loss=0.0321929083\t time_on_epoch 0 = 0.30653247493319213\n",
      "loss=0.0159933244\t time_on_epoch 10 = 0.13437715300824493\n",
      "loss=0.0127318873\t time_on_epoch 20 = 0.13457738293800503\n",
      "loss=0.0116782057\t time_on_epoch 30 = 0.134450733079575\n",
      "loss=0.0111255542\t time_on_epoch 40 = 0.13483770296443254\n",
      "loss=0.0107775138\t time_on_epoch 50 = 0.1345619229832664\n",
      "loss=0.0105360060\t time_on_epoch 60 = 0.13469453295692801\n",
      "loss=0.0103587484\t time_on_epoch 70 = 0.1347333239391446\n",
      "loss=0.0102224418\t time_on_epoch 80 = 0.13468431401997805\n",
      "loss=0.0101137650\t time_on_epoch 90 = 0.13480222295038402\n",
      "Entropy before beam search: tensor([7.9987, 7.9996, 7.9997, 7.9941], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84935a0f62754a9d827c5eff6901b657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy after beam search: tensor([7.9985, 7.9988, 7.9960, 7.9831], device='cuda:0')\n",
      "loss=0.0049323175\t time_on_epoch 100 = 0.134709634934552\n",
      "loss=0.0049000231\t time_on_epoch 110 = 0.13448562601115555\n",
      "loss=0.0048899715\t time_on_epoch 120 = 0.1345701760146767\n",
      "loss=0.0048844423\t time_on_epoch 130 = 0.13458515598904341\n",
      "loss=0.0048811730\t time_on_epoch 140 = 0.1345395160606131\n",
      "loss=0.0048786117\t time_on_epoch 150 = 0.13452253595460206\n",
      "loss=0.0048758113\t time_on_epoch 160 = 0.13480562600307167\n",
      "loss=0.0048737549\t time_on_epoch 170 = 0.13468538597226143\n",
      "loss=0.0048721806\t time_on_epoch 180 = 0.13461041601840407\n",
      "loss=0.0048709052\t time_on_epoch 190 = 0.13450881594326347\n",
      "Entropy before beam search: tensor([7.9985, 7.9988, 7.9960, 7.9831], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0ac6f2d8274ce2ac3a8a27c74d2696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy after beam search: tensor([7.9982, 7.9963, 7.9506, 7.5419], device='cuda:0')\n",
      "loss=0.0038284234\t time_on_epoch 200 = 0.13733786495868117\n",
      "loss=0.0038126490\t time_on_epoch 210 = 0.13716789591126144\n",
      "loss=0.0038079356\t time_on_epoch 220 = 0.13720458594616503\n",
      "loss=0.0038053051\t time_on_epoch 230 = 0.1372052360093221\n",
      "loss=0.0038036157\t time_on_epoch 240 = 0.1372614960419014\n",
      "loss=0.0038024541\t time_on_epoch 250 = 0.13720554602332413\n",
      "loss=0.0038016361\t time_on_epoch 260 = 0.13716848602052778\n",
      "loss=0.0038010355\t time_on_epoch 270 = 0.1372344959527254\n",
      "loss=0.0038005370\t time_on_epoch 280 = 0.13724346598610282\n",
      "loss=0.0038000721\t time_on_epoch 290 = 0.13733416702598333\n",
      "Entropy before beam search: tensor([7.9982, 7.9963, 7.9506, 7.5419], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c800e76eaa42bfb61e7ca2485cb273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy after beam search: tensor([7.9961, 7.9541, 6.9827, 3.4024], device='cuda:0')\n",
      "loss=0.0051695032\t time_on_epoch 300 = 0.16384631302207708\n",
      "loss=0.0050173219\t time_on_epoch 310 = 0.16333806398324668\n",
      "loss=0.0050003570\t time_on_epoch 320 = 0.16364737402182072\n",
      "loss=0.0049963343\t time_on_epoch 330 = 0.1634368139784783\n",
      "loss=0.0049941654\t time_on_epoch 340 = 0.16428753302898258\n",
      "loss=0.0049926795\t time_on_epoch 350 = 0.16437282401602715\n",
      "loss=0.0049915527\t time_on_epoch 360 = 0.16335051401983947\n",
      "loss=0.0049906475\t time_on_epoch 370 = 0.16335509507916868\n",
      "loss=0.0049898923\t time_on_epoch 380 = 0.16339253494516015\n",
      "loss=0.0049892443\t time_on_epoch 390 = 0.16330697503872216\n",
      "Entropy before beam search: tensor([7.9961, 7.9541, 6.9827, 3.4024], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1044dcedd04467af5092404b3440da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy after beam search: tensor([7.9856, 7.4455, 3.5256, 1.0036], device='cuda:0')\n",
      "loss=0.0076876331\t time_on_epoch 400 = 0.17860544798895717\n",
      "loss=0.0075240223\t time_on_epoch 410 = 0.1784091090084985\n",
      "loss=0.0075106094\t time_on_epoch 420 = 0.17836942803114653\n",
      "loss=0.0075060319\t time_on_epoch 430 = 0.17836837901268154\n",
      "loss=0.0075032820\t time_on_epoch 440 = 0.17839083902072161\n",
      "loss=0.0075013630\t time_on_epoch 450 = 0.17844732908997685\n",
      "loss=0.0074999108\t time_on_epoch 460 = 0.17835793900303543\n",
      "loss=0.0074987534\t time_on_epoch 470 = 0.17834684904664755\n",
      "loss=0.0074977965\t time_on_epoch 480 = 0.17841326992493123\n",
      "loss=0.0074969833\t time_on_epoch 490 = 0.17838564002886415\n",
      "Entropy before beam search: tensor([7.9856, 7.4455, 3.5256, 1.0036], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca93a4725c104e1c8fff4971e3cce7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy after beam search: tensor([7.9439, 5.7458, 1.9318, 0.6045], device='cuda:0')\n",
      "loss=0.0094117466\t time_on_epoch 500 = 0.18043602001853287\n",
      "loss=0.0092709140\t time_on_epoch 510 = 0.18020800000522286\n",
      "loss=0.0092241957\t time_on_epoch 520 = 0.18038407107815146\n",
      "loss=0.0092177592\t time_on_epoch 530 = 0.18025261105503887\n",
      "loss=0.0092142588\t time_on_epoch 540 = 0.18031079100910574\n",
      "loss=0.0092117680\t time_on_epoch 550 = 0.18030279001686722\n",
      "loss=0.0092098558\t time_on_epoch 560 = 0.18029104091692716\n",
      "loss=0.0092083175\t time_on_epoch 570 = 0.18037664098665118\n",
      "loss=0.0092070379\t time_on_epoch 580 = 0.1802686209557578\n",
      "loss=0.0092059457\t time_on_epoch 590 = 0.18062070093583316\n",
      "Entropy before beam search: tensor([7.9439, 5.7458, 1.9318, 0.6045], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092fc13a76154e0db5276e447f02fc6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy after beam search: tensor([7.8416, 4.2590, 1.3671, 0.5135], device='cuda:0')\n",
      "loss=0.0105941560\t time_on_epoch 600 = 0.18127939198166132\n",
      "loss=0.0104853044\t time_on_epoch 610 = 0.18106419208925217\n",
      "loss=0.0104659060\t time_on_epoch 620 = 0.18145907192956656\n",
      "loss=0.0104495522\t time_on_epoch 630 = 0.1815927519928664\n",
      "loss=0.0104453061\t time_on_epoch 640 = 0.18115997198037803\n",
      "loss=0.0104424215\t time_on_epoch 650 = 0.18114822206553072\n",
      "loss=0.0104402021\t time_on_epoch 660 = 0.181067563011311\n",
      "loss=0.0104384160\t time_on_epoch 670 = 0.18107824202161282\n",
      "loss=0.0104369319\t time_on_epoch 680 = 0.18131924199406058\n"
     ]
    }
   ],
   "source": [
    "quantized_weight = QuantizedWeight(\n",
    "    XTX=XTX, reference_weight=reference_weight, num_codebooks=num_codebooks,\n",
    "    nbits_per_codebook=nbits_per_codebook, scale_nbits=scale_nbits, \n",
    "    out_group_size=out_group_size, in_group_size=in_group_size,\n",
    "    verbose=True, max_iter=init_max_iter,   # faster init, not tested\n",
    ")\n",
    "run.log({\"Avg_bits\": quantized_weight.estimate_nbits_per_parameter()})\n",
    "print(\"AVG bits:\", quantized_weight.estimate_nbits_per_parameter())\n",
    "opt = torch.optim.Adam(quantized_weight.parameters(), lr=1e-4, betas=(0.0, 0.95), amsgrad=True)\n",
    "\n",
    "\n",
    "for epoch in range(1000):\n",
    "    start = time.perf_counter()\n",
    "    delta_weight = (quantized_weight() - reference_weight).double()\n",
    "    loss = (delta_weight @ XTX.double()).flatten() @ delta_weight.flatten() / len(delta_weight)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    run.log({'loss':loss.item()}, step=epoch)\n",
    "    \n",
    "    if epoch % print_frequency == 0:\n",
    "        print(f\"loss={loss.item():.10f}\\t\",\n",
    "              f\"time_on_epoch {epoch} = {time.perf_counter() - start}\")\n",
    "    if (epoch + 1) % beam_search_epochs == 0:\n",
    "        print(\"Entropy before beam search:\", _calculate_code_entropy(quantized_weight.codes))\n",
    "        code_penalties = _get_entropy_penalties_upper_bound(quantized_weight.codes, regularizer=entropy_regularizer)\n",
    "        if epoch < entropy_warmup_epochs:   # hypothesis: this could help last codebooks become more meaningful before we prune them via entropy regularizer\n",
    "            code_penalties *= 0\n",
    "            print(\"Not regularizing for epoch\", epoch)\n",
    "        quantized_weight.beam_search_update_codes_(\n",
    "            XTX, reference_weight, beam_size=beam_size, code_penalties=code_penalties,\n",
    "            dim_rng=random.Random(), verbose=True)\n",
    "        print(\"Entropy after beam search:\", _calculate_code_entropy(quantized_weight.codes))\n",
    "        \n",
    "#         if code_penalties is not None:\n",
    "#             mean_code_nbits = sum(get_mean_nbits_by_codebook(quantized_weight.codes)) / num_codebooks\n",
    "#             print(f\"mean_code_nbits {mean_code_nbits:.5f}\")\n",
    "#             run.log({'Mean codebook length nbits': mean_code_nbits}, step=epoch)\n",
    "#             if in_group_size > 1 and out_group_size > 1:\n",
    "#                 curr_avg_bits  = calc_avg_bits(num_codebooks, 1, mean_code_nbits,\n",
    "#                                      nbits_per_codebook, in_features, out_features, scale_nbits)\n",
    "#                 run.log({\"Avg_bits\": curr_avg_bits}, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5702df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
