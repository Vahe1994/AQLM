{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43871e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3\n",
      "env: TRANSFORMERS_CACHE=/mnt/LLM/hub\n",
      "env: OMP_NUM_THREADS=16\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=3\n",
    "%env TRANSFORMERS_CACHE=/mnt/LLM/hub\n",
    "%env OMP_NUM_THREADS=16\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import time\n",
    "import random\n",
    "from tqdm.auto import trange\n",
    "import ipynbname  # pip install ipynbname\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "\n",
    "from src.aq import QuantizedWeight\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "\n",
    "\n",
    "torch.set_num_threads(min(16, 16))\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_loading_dir = '/extra_disk_1/vahe1994/BRRR/layer10.self_attn.q_proj.input_activation.pt'\n",
    "num_codebooks = 2\n",
    "nbits_per_codebook = 8\n",
    "out_group_size = 1\n",
    "in_group_size = 8\n",
    "batch_size = 16384\n",
    "beam_size = 1\n",
    "beam_search_epochs = 100\n",
    "print_frequency = 10\n",
    "scale_nbits = 0    # 0 means no scales, 16 means no compression;\n",
    "codebook_values_nbits = 16  # less than 16 means we quantize codebooks as well\n",
    "init_max_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbf7e1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644d630b7df84b06ab24e7a07c6e5a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7001980d08264472a8ddd311e0f9379c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-70b-hf\", torch_dtype='auto', low_cpu_mem_usage=True)\n",
    "\n",
    "X = torch.load(input_loading_dir, map_location='cpu').float().flatten(0, -2)\n",
    "reference_weight = model.model.layers[10].self_attn.q_proj.weight.detach().to(device).float()\n",
    "\n",
    "XTX = torch.zeros(X.shape[-1], X.shape[-1], device=device, dtype=torch.float64)\n",
    "for i in range(0, len(X), batch_size):\n",
    "    x_batch = X[i: i + batch_size].cuda().double()\n",
    "    XTX.addmm_(x_batch.T, x_batch, alpha=1/len(X))\n",
    "    del x_batch\n",
    "XTX = XTX.float()\n",
    "del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e8656f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7729f03001c4b97bdfa6ad048ed2501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "initializing with kmeans:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantized_weight = QuantizedWeight(\n",
    "    reference_weight=reference_weight, num_codebooks=num_codebooks,\n",
    "    nbits_per_codebook=nbits_per_codebook, scale_nbits=scale_nbits, \n",
    "    out_group_size=out_group_size, in_group_size=in_group_size,\n",
    "    verbose=True, max_iter=init_max_iter,   # faster init, not tested\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d116640c",
   "metadata": {},
   "source": [
    "## Entropy penalty upper bounds (fixed!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59af7561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "def _calculate_code_frequencies(codes: torch.LongTensor):\n",
    "    code_counts = torch.zeros(num_codebooks, 2**nbits_per_codebook, dtype=torch.int64, device=codes.device)\n",
    "    for codebook_index in range(num_codebooks):\n",
    "        code_counts[codebook_index, :] = torch.bincount(\n",
    "            codes[..., codebook_index].flatten(), minlength=2**nbits_per_codebook)\n",
    "    return code_counts.float() / code_counts.sum(-1, keepdim=True)\n",
    "\n",
    "def _calculate_code_entropy(codes: torch.LongTensor, eps: float = 1e-20):\n",
    "    \"\"\"Calculate per-codebook code entropy measured in bits (base-2)\"\"\"\n",
    "    probs = _calculate_code_frequencies(codes)\n",
    "    logprobs = torch.log2(probs.clamp_min(eps))\n",
    "    return - torch.sum(probs * logprobs, dim=-1)\n",
    "\n",
    "import huffman\n",
    "def _get_huffman_penalties_upper_bound(codes: torch.LongTensor, regularizer: float):\n",
    "    \"\"\"Compute log-probability penalties that minimize a linearized upper bound on entropy \"\"\"\n",
    "    penalties = torch.empty(num_codebooks, 2 ** nbits_per_codebook, device=codes.device, dtype=torch.float32)\n",
    "    probs = _calculate_code_frequencies(codes)\n",
    "    \n",
    "    for codebook_index in range(num_codebooks):\n",
    "        num_codes = torch.as_tensor(codes[..., codebook_index].numel(), device=probs.device)\n",
    "        missing_value_length = torch.log2(num_codes).item()\n",
    "        \n",
    "        huffman_codes = huffman.codebook([(i, probs[codebook_index, i].item()) for i in range(2 ** nbits_per_codebook)])\n",
    "        code_lengths = torch.as_tensor([\n",
    "            len(huffman_codes.get(i, missing_value_length)) for i in range(2 ** nbits_per_codebook)],\n",
    "            device=probs.device, dtype=torch.float32)\n",
    "        penalties[codebook_index] = (regularizer / probs.shape[-1]) * code_lengths\n",
    "    return penalties\n",
    "\n",
    "def _get_entropy_penalties_upper_bound(codes: torch.LongTensor, regularizer: float, eps: Optional[float] = None):\n",
    "    \"\"\"Compute log-probability penalties that minimize a linearized upper bound on entropy \"\"\"\n",
    "    probs = _calculate_code_frequencies(codes)\n",
    "    num_codes = torch.as_tensor(codes[..., 0].numel(), device=probs.device)\n",
    "    if eps is None:\n",
    "        eps = 1. / num_codes\n",
    "    logprobs = torch.log2(probs.clamp_min(eps))\n",
    "    return (- regularizer / probs.shape[-1]) * logprobs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc0ca4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.9844, 7.9880], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_calculate_code_entropy(quantized_weight.get_codes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce3f160f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(64.0238, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalties = _get_entropy_penalties_upper_bound(quantized_weight.get_codes(), regularizer=1.0)\n",
    "penalties[0, quantized_weight.get_codes()[0]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "767ad688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(64.0391, device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalties = _get_huffman_penalties_upper_bound(quantized_weight.get_codes(), regularizer=1.0)\n",
    "penalties[0, quantized_weight.get_codes()[0]].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f1bff5",
   "metadata": {},
   "source": [
    "### Copy of beam_search_l2 with entropy penalties introduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aefcb304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Beam search that minimizes ||Wref - Wq||^2 w.r.t. Wq\"\"\"\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src.utils import _dequantize_weight, maybe_script\n",
    "\n",
    "\n",
    "@torch.inference_mode\n",
    "def beam_search_optimal_codes(\n",
    "    reference_weight: torch.Tensor,\n",
    "    codebooks: torch.Tensor,\n",
    "    prev_codes: torch.Tensor,\n",
    "    scales: Optional[torch.Tensor],\n",
    "    beam_size: int,\n",
    "    stochastic_rounding_tau: float = 0.0,\n",
    "    chunk_size_bytes: int = 2**32,\n",
    "    penalties: Optional[torch.Tensor] = None,\n",
    "    penalty_weights: Optional[torch.Tensor] = None,\n",
    "    dim_rng: Optional[random.Random] = None,\n",
    "    force_update: bool = False,\n",
    "    max_update_fraction: float = 1.0,\n",
    "    code_selection_temperature: float = 0,\n",
    "    trust_ratio: Optional[float] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Update codes using beam search to minimize L2 error in code values (regardless of activations)\n",
    "    :param reference_weight: a target for L2 error, [out_features, in_features]\n",
    "    :param codebooks: look-up tables of codes, shape: [num_codebooks, codebook_size, out_group_size, in_group_size]\n",
    "    :param prev_codes: previous-best integer weight codes, shape: [num_output_groups, num_input_groups, num_codebooks]\n",
    "    :param scales: weight will be multiplied by this factor, shape = [num_output_groups, num_input_groups or 1, 1, 1]\n",
    "    :param dim_rng: a source of randomness to (optionally) shuffle the order in which the beam search runs\n",
    "      None = update dimensions and codebooks in their natural order (0, 1, ..., n)\n",
    "      random.Random(optional_seed) = shuffle dimensions at random, optionally using the specified seed\n",
    "\n",
    "    :param beam_size: consider up to this many best encoding combinations\n",
    "    :param stochastic_rounding_tau: if positive, each time the algorithm chooses a code, it will have a probability\n",
    "        of replacing it with the second-best choice. If the two best codes increase the error by delta1 and delta2,\n",
    "        then the probability of choosing each code is P_i = delta_i ^ -1/tau / (sum_j_in_choices delta_j ^ -1/tau).\n",
    "        Note that if there is a code that has zero error, the algorithm will choose allways choose such a code\n",
    "    :param chunk_size_bytes: process this many candidates at a time; reduce to save memory\n",
    "    :param penalties: additional penalty to loss for picking a given code, [num_codebooks, codebook_size]\n",
    "    :param penalty_weigths: multiplicative scale to penalties (same shape as codes) so that the final objective is \n",
    "        ||reference_weight - dequantize(codes)||^2 + sum_[i,j,c] penalties[codes[i, j, c]] * penalty_weights[i,j,c]\n",
    "    \n",
    "    :param force_update: if True, the algorithm will force codes to change even if code is optimal in terms\n",
    "     of mean squared error. By default, the algorithm forces *all* weights to update this way, which may change weights\n",
    "     too much. To limit the numer of updated weights, set max_code_change and trust_ratio.\n",
    "    :param max_update_fraction: the maximum portion of discrete code groups that *can* be updated;\n",
    "        By default, all codes can be updated. If < 1, only this portion of all code groups is allowed to update.\n",
    "        The algorithm selects the codes for update based on the difference between de-quantized and reference_weight.\n",
    "        If there are multiple codebooks, changing any one code responsible for the group counts as code group changed.\n",
    "        Note that small max_code_change also speeds up computation since not all codes need beam search.\n",
    "        If the number of weights do not divide evenly, the algoritm will round the number of updates up.\n",
    "    :param code_selection_temperature: only used if max_code_change > 1; by default, prioritize updating the codes with\n",
    "        the largest delta = ||(reference_weight - quantized_weight) * mask_only_weights_that_depend_on_this_code||_2 .\n",
    "        If temperature > 0, the updated codes are instead *sampled* at random, proportionally to delta^(1/temperature) .\n",
    "    :param trust_ratio: if not None, the algorithm only admits code changes as long as they do not change too much.\n",
    "        Formally, ||new_quantized_weight - prev_quantized_weight|| / ||prev_quantized_weight|| <= trust_ratio\n",
    "        If this is not true, the algorithm will reset some of the new quantized weights to their old values until the\n",
    "        constraint becomes satisfied. The algorithm still prioritizes changes to weights with largest delta (see above).\n",
    "        If code_change_temperature > 0, the algorithm instead samples which weights to change with the same probability.\n",
    "        The algorithm will always allow changing exactly *one* code in excess of trust ratio to ensure that at least\n",
    "        one weight is updated. If both this and max_code_change is set, both these constraints are enforced.\n",
    "    :return: the best quantization codes found within constraints, same shape as prev_codes\n",
    "\n",
    "    \"\"\"\n",
    "    assert 0 < max_update_fraction <= 1 and (trust_ratio is None or trust_ratio > 0)\n",
    "    # reshape references, codes and codebooks so they are no longer group-wise\n",
    "    num_output_groups, num_input_groups, num_codebooks = prev_codes.shape\n",
    "    _num_codebooks, codebook_size, out_group_size, in_group_size = codebooks.shape\n",
    "    \n",
    "    if penalty_weights is not None:\n",
    "        assert penalties is not None\n",
    "        assert penalty_weights.shape == codes.shape  # (num_output_groups, num_input_groups, num_codebooks)\n",
    "    if penalties is not None:\n",
    "        assert penalties.shape == codebooks.shape[:2]  # (num_codebooks, codebook_size)\n",
    "        if penalty_weights is None:\n",
    "            penalty_weights = torch.ones_like(prev_codes, dtype=codebooks.dtype)\n",
    "        penalty_weights = penalty_weights / scales.square().squeeze(-1)\n",
    "        # why: in the inner beam search, we minimize un-scaled MSE; dividing penalty weights by scales^2 is ...\n",
    "        # ... equivalent to optimizing scaled MSE; weights with larger scales are more important.\n",
    "        \n",
    "        penalty_weights = penalty_weights.flatten(0, 1)  # [num_groups, num_codebooks]\n",
    "\n",
    "    flat_unscaled_reference = reference_weight.reshape(\n",
    "        num_output_groups, out_group_size, num_input_groups, in_group_size\n",
    "    ).permute(\n",
    "        0, 2, 1, 3\n",
    "    )  # [num_output_groups, num_input_groups, out_group_size, in_group_size]\n",
    "    if scales is not None:\n",
    "        flat_unscaled_reference = flat_unscaled_reference / scales\n",
    "        # divide by scales; the resulting problem is equivalent to multiplying dequantized weight\n",
    "    flat_unscaled_reference = flat_unscaled_reference.flatten(2, 3).flatten(0, 1) # [num_output_groups*num_input_groups,out_group_size*in_group_size]\n",
    "    flat_prev_codes = prev_codes.flatten(0, -2) \n",
    "    flat_codebooks = codebooks.flatten(-2, -1).detach()\n",
    "    dim_order = list(range(num_codebooks))\n",
    "    if dim_rng is not None:\n",
    "        dim_rng.shuffle(dim_order)\n",
    "\n",
    "    def _update_flat_codes(_flat_reference, _flat_codes):\n",
    "        \"\"\"update _flat_codes [num_groups, num_codebooks] to approximate _flat_reference [num_groups, group_size]\"\"\"\n",
    "        if num_codebooks == 1 and beam_size == 1 and stochastic_rounding_tau == 0 and not force_update:\n",
    "            # a faster algorithm for a special case of one codebook\n",
    "            return _greedy_find_best_codes(\n",
    "                reference=_flat_reference,\n",
    "                codebook=flat_codebooks[0],\n",
    "                chunk_size_values=chunk_size_bytes // _flat_reference[0, 0].nbytes,\n",
    "                code_dtype=prev_codes.dtype,\n",
    "                penalties=penalties,\n",
    "                penalty_weights=penalty_weights,\n",
    "            )\n",
    "        else:\n",
    "            return _beam_search_update_codes_groupwise(\n",
    "                reference=_flat_reference,\n",
    "                codebooks=flat_codebooks,\n",
    "                codes=_flat_codes,\n",
    "                beam_size=beam_size,\n",
    "                stochastic_rounding_tau=stochastic_rounding_tau,\n",
    "                force_update=force_update,\n",
    "                chunk_size_values=chunk_size_bytes // _flat_reference[0, 0].nbytes,\n",
    "                dim_order=dim_order,\n",
    "                penalties=penalties,\n",
    "                penalty_weights=penalty_weights,\n",
    "            )\n",
    "\n",
    "    def _groupwise_squared_norms(delta: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Given a matrix delta [out_features, in_features], compute a tensor [num_output_groups, num_input_groups] that\n",
    "        contains the squared sum of elements of delta from each tile of (out_group_size, in_group_size) values.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            delta.view(delta.shape[0] // out_group_size, out_group_size, delta.shape[1] // in_group_size, in_group_size)\n",
    "            .square()\n",
    "            .sum(dim=(1, 3))\n",
    "        )\n",
    "\n",
    "    flat_indices_to_update = prev_dequantized_weight = None\n",
    "    if max_update_fraction < 1 or trust_ratio is not None:\n",
    "        # precompute ordered code indices to be used for constraints on the number of updates\n",
    "        prev_dequantized_weight = _dequantize_weight(prev_codes, codebooks, scales)\n",
    "        num_codes_to_update = int(math.ceil(max_update_fraction * num_output_groups * num_input_groups))\n",
    "        difference_with_reference_squared_norms = _groupwise_squared_norms(reference_weight - prev_dequantized_weight)\n",
    "        # ^-- [num_output_groups, num_input_groups]\n",
    "        if code_selection_temperature > 0:\n",
    "            flat_indices_to_update = torch.pow(\n",
    "                difference_with_reference_squared_norms.flatten(),\n",
    "                0.5 / code_selection_temperature,\n",
    "                # note: temperature is multuplied by 0.5 because sampling is proportional to norms without square\n",
    "            ).multinomial(num_samples=num_codes_to_update, replacement=False)\n",
    "        else:\n",
    "            flat_indices_to_update = torch.topk(\n",
    "                difference_with_reference_squared_norms.flatten(), k=num_codes_to_update, largest=True, sorted=True\n",
    "            ).indices\n",
    "\n",
    "    if max_update_fraction == 1:\n",
    "        print(f\"{flat_unscaled_reference.shape=}\",f\"{flat_prev_codes.shape=}\",)\n",
    "        print(f\"{penalties.shape=}\",f\"{penalty_weights.shape=}\")\n",
    "        flat_new_codes = _update_flat_codes(flat_unscaled_reference, flat_prev_codes)\n",
    "    else:\n",
    "        # may be penalty_weights -> penalty_weights[flat_indices_to_update]\n",
    "        penalty_weights = penalty_weights[flat_indices_to_update]\n",
    "        flat_new_codes = flat_prev_codes.index_put(  # note: this is an out-of-place op that does not modify prev codes\n",
    "            (flat_indices_to_update[:, None], torch.arange(num_codebooks, device=codebooks.device)[None, :]),\n",
    "            _update_flat_codes(\n",
    "                flat_unscaled_reference[flat_indices_to_update], flat_prev_codes[flat_indices_to_update]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    if trust_ratio is not None:\n",
    "        assert isinstance(flat_indices_to_update, torch.Tensor) and isinstance(prev_dequantized_weight, torch.Tensor)\n",
    "        new_dequantized_weight = _dequantize_weight(flat_new_codes.view_as(prev_codes), codebooks, scales)\n",
    "        weight_change_squared_norms = _groupwise_squared_norms(new_dequantized_weight - prev_dequantized_weight)\n",
    "        # ^-- shape: [num_output_groups, num_input_groups]\n",
    "\n",
    "        flat_ordered_weight_change_squared_norms = weight_change_squared_norms.flatten()[flat_indices_to_update]\n",
    "        flat_ordered_cumulative_norms = flat_ordered_weight_change_squared_norms.cumsum(0).sqrt()\n",
    "        # [num_codes_to_update]\n",
    "\n",
    "        num_codes_selected = 1 + torch.searchsorted(\n",
    "            flat_ordered_cumulative_norms, trust_ratio * prev_dequantized_weight.norm(), side=\"left\"\n",
    "        )\n",
    "        truncated_flat_indices_to_update = flat_indices_to_update[:num_codes_selected]  # sorted most to least important\n",
    "        flat_new_codes = flat_prev_codes.index_put(  # <-- note: this is an out-of-place operation\n",
    "            (truncated_flat_indices_to_update[:, None], torch.arange(num_codebooks, device=codebooks.device)[None, :]),\n",
    "            flat_new_codes[truncated_flat_indices_to_update],\n",
    "        )\n",
    "    return flat_new_codes.view_as(prev_codes)\n",
    "\n",
    "\n",
    "@maybe_script\n",
    "def _beam_search_update_codes_groupwise(\n",
    "    reference: torch.Tensor,\n",
    "    codebooks: torch.Tensor,\n",
    "    codes: torch.Tensor,\n",
    "    *,\n",
    "    beam_size: int,\n",
    "    stochastic_rounding_tau: float,\n",
    "    chunk_size_values: int,\n",
    "    dim_order: Optional[List[int]],\n",
    "    force_update: bool,\n",
    "    penalties: Optional[torch.Tensor],\n",
    "    penalty_weights: Optional[torch.Tensor],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    :param reference: [num_groups, group_size]\n",
    "    :param codes: [num_groups, num_codebooks]\n",
    "    :param codebooks: [num_codebooks, codebook_size, group_size]\n",
    "    :returns: [num_groups, num_codebooks]\n",
    "    \"\"\"\n",
    "    assert (penalty_weights is None) == (penalties is None)\n",
    "    if penalty_weights is not None:\n",
    "        assert penalties is not None\n",
    "        assert penalty_weights.shape == (len(reference), codes.shape[-1]), penalty_weights.shape\n",
    "    if stochastic_rounding_tau > 0:\n",
    "        assert beam_size >= 2, \"with stochastic rounding, we need at least 2 hypotheses to choose from\"\n",
    "\n",
    "    prev_codes = codes\n",
    "    device = reference.device\n",
    "    num_groups, group_size = reference.shape\n",
    "    num_codebooks, codebook_size, group_size = codebooks.shape\n",
    "    codebook_offsets = torch.arange(0, num_codebooks * codebook_size, codebook_size, device=device)  # [num_codebooks]\n",
    "    original_dequantized_vectors = F.embedding_bag(\n",
    "        codes + codebook_offsets, codebooks.flatten(0, 1), mode=\"sum\"\n",
    "    )  # [num_groups, group_size]\n",
    "    if dim_order is None:\n",
    "        dim_order = list(range(num_codebooks))\n",
    "\n",
    "    code_norms_sq = codebooks.square().sum(-1)  # [num_codebooks, codebook_size]\n",
    "    beam_codes = codes.clone().unsqueeze(1)  # [num_groups, current_beam_size, num_codebooks]\n",
    "    residue = (reference - original_dequantized_vectors).view(num_groups, 1, group_size)\n",
    "    # shape: [num_groups, current_beam_size, group_size]\n",
    "    direction = residue.clone().view(num_groups, group_size) if force_update else torch.empty(0)\n",
    "\n",
    "    for i, codebook_index in enumerate(dim_order):\n",
    "        current_beam_size = residue.shape[1]\n",
    "        is_last_step = i == len(dim_order) - 1\n",
    "        # ^-- [num_groups, current_beam_size, group_size]\n",
    "        residue = residue + F.embedding(beam_codes[..., codebook_index], codebooks[codebook_index, ...])\n",
    "        if beam_size > 1 or stochastic_rounding_tau > 0:\n",
    "            residue_norms_sq = residue.square().sum(-1).unsqueeze(-1)  # [num_groups, current beam size, 1]\n",
    "        else:\n",
    "            residue_norms_sq = torch.empty(0, device=device)  # when doing greedy search, these are const\n",
    "\n",
    "        if not is_last_step:\n",
    "            target_num_candidates = beam_size + int(stochastic_rounding_tau > 0)\n",
    "        else:\n",
    "            target_num_candidates = 2 if stochastic_rounding_tau > 0 or force_update else 1\n",
    "\n",
    "        flat_best_indices = torch.empty(num_groups, target_num_candidates, device=device, dtype=codes.dtype)\n",
    "        chunk_size_rows = chunk_size_values // (codebook_size * current_beam_size) // 32\n",
    "        for chunk_start in range(0, num_groups, chunk_size_rows):\n",
    "            chunk_end = min(chunk_start + chunk_size_rows, num_groups)\n",
    "            scores = torch.matmul(residue[chunk_start:chunk_end], codebooks[codebook_index].T)\n",
    "            if beam_size > 1 or stochastic_rounding_tau > 0:\n",
    "                scores = residue_norms_sq[chunk_start:chunk_end] - 2 * scores + code_norms_sq[codebook_index]\n",
    "            else:\n",
    "                scores = -2 * scores + code_norms_sq[codebook_index]  # residue norms are const(j)\n",
    "            # ^-- [num_groups_chunk, beam_size, codebook_size]\n",
    "            if penalty_weights is not None:\n",
    "                assert isinstance(penalties, torch.Tensor)\n",
    "                assert isinstance(penalty_weights, torch.Tensor)\n",
    "                scores = scores.add_(\n",
    "                    penalties[codebook_index, None, None, :] * \n",
    "                    penalty_weights[chunk_start: chunk_start + chunk_size_rows, codebook_index, None, None]\n",
    "                )\n",
    "        \n",
    "\n",
    "            flat_best_losses_chunk, flat_best_indices_chunk = torch.topk(\n",
    "                scores.flatten(1, 2),\n",
    "                k=target_num_candidates,\n",
    "                largest=False,\n",
    "                sorted=is_last_step or beam_size > 1 or stochastic_rounding_tau > 0,\n",
    "            )  # [num_groups_chunk, target_num_candidates]\n",
    "\n",
    "            if stochastic_rounding_tau > 0:\n",
    "                errors = flat_best_losses_chunk.relu().sqrt()  # non-squared errors\n",
    "                scores = torch.pow(errors / errors.sum(-1, keepdim=True), -1 / stochastic_rounding_tau)\n",
    "                # ^-- [num_groups_chunk, beam_size + 1]\n",
    "                keep_prob = scores[:, :-1] / (scores[:, :-1] + scores[:, 1:])  # [num_groups, k_best]\n",
    "                keep_prob = torch.where(torch.isinf(scores[:, :-1]), 1.0, keep_prob)\n",
    "                keep = torch.less_equal(torch.rand_like(keep_prob), keep_prob)\n",
    "                flat_best_indices_chunk = torch.where(\n",
    "                    keep, flat_best_indices_chunk[:, :-1], flat_best_indices_chunk[:, 1:]\n",
    "                )\n",
    "\n",
    "            flat_best_indices[chunk_start:chunk_end] = flat_best_indices_chunk\n",
    "\n",
    "        arange_num_groups = torch.arange(num_groups, device=device)\n",
    "        best_hypo_source_ids = flat_best_indices // codebook_size\n",
    "        best_hypo_codes = flat_best_indices % codebook_size\n",
    "        beam_codes = beam_codes[arange_num_groups[:, None], best_hypo_source_ids, :]\n",
    "        beam_codes[:, :, codebook_index] = best_hypo_codes.to(beam_codes.dtype)\n",
    "        # ^-- [num_groups, beam_size, num_codebooks]\n",
    "\n",
    "        if not is_last_step:\n",
    "            residue = residue - F.embedding(beam_codes[..., codebook_index], codebooks[codebook_index, ...])\n",
    "\n",
    "    if force_update:\n",
    "        assert beam_codes.shape[1] == 2\n",
    "        best_codes = beam_codes[:, 0, :]\n",
    "        second_best_codes = beam_codes[:, 1, :]\n",
    "        best_code_changed = torch.ne(best_codes, prev_codes).any(dim=-1)\n",
    "        return torch.where(best_code_changed.unsqueeze(-1), best_codes, second_best_codes)\n",
    "    else:\n",
    "        return beam_codes[:, 0, :]\n",
    "\n",
    "\n",
    "@maybe_script\n",
    "def _greedy_find_best_codes(\n",
    "    reference: torch.Tensor, codebook: torch.Tensor,\n",
    "    penalties: Optional[torch.Tensor],\n",
    "    penalty_weights: Optional[torch.Tensor],\n",
    "    chunk_size_values: int, code_dtype: torch.dtype,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    :param reference: [num_groups, group_size]\n",
    "    :param codebook: [codebook_size, group_size]\n",
    "    :param penalties: [codebook_size, 1]\n",
    "    :param penalty_weights: [num_groups, 1]\n",
    "    :param chunk_size_values: how many values can be materialized in memory simultaneously\n",
    "    :parma code_dtype the dtype of optimal codes returned by this function\n",
    "    :returns: codes [num_groups, 1]\n",
    "    \"\"\"\n",
    "    assert (penalty_weights is None) == (penalties is None)\n",
    "    if penalty_weights is not None:\n",
    "        assert penalties is not None\n",
    "        assert penalty_weights.shape == (len(reference), 1)\n",
    "        penalties = penalties.flatten() # [codebook_size]\n",
    "        penalty_weights = penalty_weights.flatten() # [num_groups]\n",
    "    codebook_t = codebook.T.contiguous()\n",
    "    chunk_size = chunk_size_values // len(codebook)\n",
    "    codebook_norms_sq = codebook.square().sum(dim=-1)\n",
    "    new_codes = torch.empty((len(reference),), dtype=code_dtype, device=reference.device)\n",
    "    for chunk_start in range(0, len(reference), chunk_size):\n",
    "        scores = torch.addmm(\n",
    "            codebook_norms_sq[None], reference[chunk_start : chunk_start + chunk_size], codebook_t, alpha=-2\n",
    "        )  # ||quantized^2|| - 2 * <reference * codebook> + omitted_as_const[ || reference ||^2 ]\n",
    "        if penalty_weights is not None:\n",
    "            assert isinstance(penalties, torch.Tensor)\n",
    "            assert isinstance(penalty_weights, torch.Tensor)\n",
    "            scores = scores.add_(penalties[None, :] * penalty_weights[chunk_start: chunk_start + chunk_size, None])\n",
    "        new_codes[chunk_start : chunk_start + chunk_size] = scores.argmin(-1)\n",
    "    return new_codes.unsqueeze(-1)\n",
    "\n",
    "\n",
    "def _find_optimal_codebooks(\n",
    "    reference: torch.Tensor,\n",
    "    codebooks: torch.Tensor,\n",
    "    codes: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    num_samples = len(reference)\n",
    "    num_codebooks, codebook_size, group_size = codebooks.shape\n",
    "\n",
    "    # compute optimal codebooks via linsolve\n",
    "    codebook_offsets = torch.arange(num_codebooks, device=codes.device) * codebook_size\n",
    "    code_indicators = torch.sparse_coo_tensor(\n",
    "        indices=torch.stack(\n",
    "            [\n",
    "                torch.arange(num_samples * num_codebooks, device=codes.device) // num_codebooks,\n",
    "                (codes + codebook_offsets).flatten(),\n",
    "            ],\n",
    "            0,\n",
    "        ),\n",
    "        values=torch.ones(num_samples * num_codebooks, device=codes.device),\n",
    "        size=(num_samples, num_codebooks * codebook_size),\n",
    "    )\n",
    "    cooc = (code_indicators.T @ code_indicators).coalesce()\n",
    "    rhs = code_indicators.T @ reference\n",
    "\n",
    "    try:\n",
    "        cooc = cooc.to_dense()\n",
    "        cooc[torch.arange(len(cooc)), torch.arange(len(cooc))].clamp_min_(1.0)\n",
    "        optimal_codebooks = (torch.linalg.lstsq(cooc, rhs)).solution.reshape(num_codebooks, codebook_size, group_size)\n",
    "    except Exception as e:\n",
    "        print(f\"Linsolve failed with {e}\")\n",
    "        optimal_codebooks = codebooks\n",
    "    return optimal_codebooks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2df439",
   "metadata": {},
   "source": [
    "## Demo: optimize entropy\n",
    "\n",
    "* (not tested for >1 codebooks!)\n",
    "* (not tested with user-defined penalty weights!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b87f074a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ad8e054d124d6cbd4fecd818497131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "initializing with kmeans:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantized_weight = QuantizedWeight(\n",
    "    reference_weight=reference_weight, num_codebooks=num_codebooks,\n",
    "    nbits_per_codebook=nbits_per_codebook, scale_nbits=scale_nbits, \n",
    "    out_group_size=out_group_size, in_group_size=in_group_size,\n",
    "    verbose=True, max_iter=init_max_iter,   # faster init, not tested\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18887712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy: tensor([7.9842, 7.9918], device='cuda:0')\n",
      "entropy: tensor([7.9827, 7.9753], device='cuda:0')\n",
      "entropy: tensor([7.9825, 7.7727], device='cuda:0')\n",
      "entropy: tensor([7.9787, 7.1288], device='cuda:0')\n",
      "entropy: tensor([7.9732, 7.0851], device='cuda:0')\n",
      "entropy: tensor([7.9714, 7.0846], device='cuda:0')\n",
      "entropy: tensor([7.9709, 7.0846], device='cuda:0')\n",
      "entropy: tensor([7.9707, 7.0846], device='cuda:0')\n",
      "entropy: tensor([7.9706, 7.0846], device='cuda:0')\n",
      "entropy: tensor([7.9706, 7.0846], device='cuda:0')\n",
      "entropy: tensor([7.9706, 7.0846], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('entropy:', _calculate_code_entropy(quantized_weight.get_codes()))\n",
    "\n",
    "for i in range(10):\n",
    "#     quantized_weight.get_codes()[...] =  beam_search_optimal_codes(\n",
    "#         reference_weight, quantized_weight.get_codebooks(), quantized_weight.get_codes(), quantized_weight.get_scales(),\n",
    "#         penalties=_get_entropy_penalties_upper_bound(quantized_weight.get_codes(), regularizer=0.1),\n",
    "#         beam_size=1,\n",
    "#     )\n",
    "    prev_codes = quantized_weight.get_codes().clone()\n",
    "    new_codes = quantized_weight.beam_search_update_codes_(\n",
    "                        reference_weight=reference_weight,\n",
    "                        beam_size=1,\n",
    "                        stochastic_rounding_tau=0.0,\n",
    "                        max_update_fraction=0.2,\n",
    "                        force_update=False,\n",
    "                        code_selection_temperature=0,\n",
    "                        trust_ratio=None,\n",
    "                        dim_rng=random.Random(None),\n",
    "                        penalties=_get_entropy_penalties_upper_bound(prev_codes,  regularizer=0.1)\n",
    "                    )\n",
    "\n",
    "    print('entropy:', _calculate_code_entropy(quantized_weight.get_codes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42abf7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_entropy_penalties_upper_bound(quantized_weight.get_codes(), regularizer=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "963043b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8192, 1024, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_weight.get_codes().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a0e5b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8192, 1024, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(quantized_weight.get_codes()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34fc2242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ade1981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
