{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43871e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3\n",
      "env: TRANSFORMERS_CACHE=/mnt/LLM/hub\n",
      "env: HF_HOME=/mnt/LLM/\n",
      "env: OMP_NUM_THREADS=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vahe1994/anaconda3/lib/python3.9/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=3\n",
    "%env TRANSFORMERS_CACHE=/mnt/LLM/hub\n",
    "%env HF_HOME=/mnt/LLM/\n",
    "%env OMP_NUM_THREADS=16\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import time\n",
    "import random\n",
    "from tqdm.auto import trange, tqdm\n",
    "import numpy as np\n",
    "import ipynbname  # pip install ipynbname\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "\n",
    "from src.aq import QuantizedWeight, QuantizedLinear\n",
    "from src.modelutils import get_model\n",
    "from src.datautils import get_loaders\n",
    "from convert_legacy_model_format import load_quantized_model_with_old_pickle\n",
    "\n",
    "\n",
    "torch.set_num_threads(16)\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbf7e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    base_model = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "    quant_model = \"/extra_disk_1/vahe1994/AQ/tinyllama-3t-2x8g8/\"\n",
    "    dtype = 'bfloat16'\n",
    "    model_seqlen = 1024  # can be 2048 for 1.1B, 4096-8192 for larger models\n",
    "    device_map = 'auto'\n",
    "    \n",
    "    dataset = 'pajama'\n",
    "    step_nsamples = 256\n",
    "    total_nsamples = 2560#2560\n",
    "    seed = 42\n",
    "    beam_size = 4\n",
    "    stochastic_rounding_tau = 0.0\n",
    "    max_code_change_per_step=0.1 #1e-3\n",
    "    code_trust_ratio=0.1\n",
    "    entropy_reg=0.1\n",
    "    code_selection_temperature=100\n",
    "    \n",
    "    code_lr = 1e-3\n",
    "    code_lr_plateau_scale = 0.5\n",
    "    code_betas = (0.0, 0.95)\n",
    "    delta_decay = 1.0\n",
    "    codebook_lr = 1e-5\n",
    "    codebook_betas = (0.9, 0.95)\n",
    "    codebook_grad_accumulation_steps = 8\n",
    "    \n",
    "    \n",
    "    autocast_dtype = torch.bfloat16  # bfloat16 or None (not using grad scaler!)\n",
    "    training_dtype = torch.float32\n",
    "    gradient_checkpointing = False\n",
    "    devices = [device]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfd37666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading red_pajama from togethercomputer/RedPajama-Data-1T-Sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vahe1994/anaconda3/lib/python3.9/site-packages/datasets/load.py:1429: FutureWarning: The repository for togethercomputer/RedPajama-Data-1T-Sample contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/togethercomputer/RedPajama-Data-1T-Sample\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from pajama; len(data)=2560 sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "train_data = get_loaders(\n",
    "    args.dataset,\n",
    "    nsamples=args.total_nsamples,\n",
    "    seed=args.seed,\n",
    "    model_path=args.base_model,\n",
    "    seqlen=args.model_seqlen,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10eb1e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model ...\n",
      "Model loaded sucсessfully ...\n",
      "Loading pretrained model ...\n",
      "Model loaded sucсessfully ...\n",
      "Initializing model with random weights...\n",
      "Loading quantized model ...\n",
      "Model loaded sucсessfully ...\n",
      "found 154 quantized weight matrices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vahe1994/code/entropy/AQLM/notebooks/../convert_legacy_model_format.py:51: UserWarning: You should be ashamed of yourself.\n",
      "  warnings.warn(\"You should be ashamed of yourself.\")\n"
     ]
    }
   ],
   "source": [
    "base_model = get_model(args.base_model, None, args.dtype, args.device_map)\n",
    "if not args.device_map:\n",
    "    base_model = base_model.to(device)\n",
    "\n",
    "quantized_model = load_quantized_model_with_old_pickle(\n",
    "    args.base_model, args.quant_model, dtype=args.dtype, device_map=args.device_map)\n",
    "if not args.device_map:\n",
    "    quantized_model = quantized_model.to(device)\n",
    "quantized_model = quantized_model.to(args.training_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1242fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pv_utils import create_dequantized_model\n",
    "dequantized_model, master_parameters = create_dequantized_model(\n",
    "    quantized_model, reuse_non_quantized=True, dequantized_dtype=args.autocast_dtype\n",
    ")\n",
    "for param in dequantized_model.parameters():\n",
    "    param.data = param.data.to(args.autocast_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50e0731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pv_optimizer import StraightThroughAdamW\n",
    "\n",
    "optimizer = StraightThroughAdamW(\n",
    "    named_dequantized_params=dict(dequantized_model.named_parameters()),\n",
    "    named_quantized_params=master_parameters,\n",
    "    \n",
    "    update_codes=dict(lr=args.code_lr, betas=args.code_betas),\n",
    "#     update_codebooks_and_scales=dict(lr=args.codebook_lr, betas=args.codebook_betas),\n",
    "#     update_non_quantized_parameters=dict(lr=args.codebook_lr, betas=args.codebook_betas),\n",
    "    code_trust_ratio=args.code_trust_ratio,\n",
    "    beam_size=args.beam_size,\n",
    "    max_code_change_per_step=args.max_code_change_per_step,\n",
    "    stochastic_rounding_tau=args.stochastic_rounding_tau,\n",
    "    straight_through_buffer_dtype=torch.float32,\n",
    "    entropy_reg=args.entropy_reg,\n",
    "    code_selection_temperature=args.code_selection_temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d1b3654",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.gradient_checkpointing:\n",
    "    quantized_model.gradient_checkpointing_enable()\n",
    "    quantized_model.enable_input_require_grads()\n",
    "    for module in quantized_model.modules():\n",
    "        if isinstance(module, QuantizedLinear):\n",
    "            module.use_checkpoint = True\n",
    "    dequantized_model.gradient_checkpointing_enable()\n",
    "    dequantized_model.enable_input_require_grads()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4e5d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_one_step(args, base_model, dequantized_model, optimizer, train_data, **kwargs):\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    with tqdm(train_data, desc=\"V step\") as progress:\n",
    "\n",
    "        total_loss = 0.0\n",
    "        for i, batch in enumerate(progress):\n",
    "            batch = torch.as_tensor(batch, device=device)\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = base_model(batch).logits\n",
    "            student_logits = dequantized_model(batch).logits  # forward accumulates XTX statistics\n",
    "            loss = kl_div(student_logits, teacher_logits)\n",
    "            (loss / len(train_data)).backward()  # backward accumulates gradient\n",
    "            total_loss = loss.item() / (i + 1) + total_loss * i / (i + 1)\n",
    "            progress.desc = f\"V step: accumulating gradients, loss = {total_loss:.9f}\"\n",
    "            del student_logits, teacher_logits, loss\n",
    "    optimizer.step(**kwargs)\n",
    "    optimizer.zero_grad(set_to_none=True)  # reset statistics for the next step\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def kl_div(student_hiddens, teacher_hiddens):\n",
    "    C = student_hiddens.shape[-1]  # num classes\n",
    "    return F.kl_div(\n",
    "        input=F.log_softmax(student_hiddens.view(-1, C), dim=-1),\n",
    "        target=F.log_softmax(teacher_hiddens.view(-1, C), dim=-1),\n",
    "        log_target=True,\n",
    "        reduction=\"batchmean\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3eb28253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from wikitext2; len(data)=1 sequences\n"
     ]
    }
   ],
   "source": [
    "eval_data = get_loaders(\n",
    "    'wikitext2',\n",
    "    seed=args.seed,\n",
    "    model_path=args.base_model,\n",
    "    seqlen=args.model_seqlen,\n",
    "    eval_mode=True,\n",
    ")\n",
    "\n",
    "@torch.inference_mode()\n",
    "def eval_ppl_naive(model, eval_data):\n",
    "    eval_inps = [\n",
    "        eval_data[:, start: start + args.model_seqlen] for start in range(0, eval_data.shape[1], args.model_seqlen)\n",
    "    ]\n",
    "    total_tokens = 0\n",
    "    nlls = []\n",
    "    for input_ids in tqdm(eval_inps):\n",
    "        input_ids = input_ids.to(device)\n",
    "        lm_logits = model(input_ids).logits\n",
    "\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "        shift_labels = input_ids[:, 1:]\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        neg_log_likelihood = loss.float() * args.model_seqlen\n",
    "        nlls.append(neg_log_likelihood)\n",
    "        total_tokens += shift_labels.numel()\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / total_tokens)\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a800a779",
   "metadata": {},
   "outputs": [],
   "source": [
    "POINTER = 0\n",
    "def next_train_data():\n",
    "    global POINTER\n",
    "    batch = []\n",
    "    for i in range(args.step_nsamples):\n",
    "        batch.append(train_data[POINTER % len(train_data)])\n",
    "        POINTER += 1\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180c8433",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0f6f5447dc046f6a1bbcd5fd3554cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikitext2 ppl (naive) 18.941404342651367\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db834f988f84427b580bfa9420f6e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "V step:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG entropy(not correct): 7.960147365347131\n",
      "train loss: 0.6543046110309659\n",
      "STEP 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2ccd29b7f2477db1f94d1388589d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "V step:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG entropy(not correct): 7.9397684598897955\n",
      "train loss: 0.5452335950685666\n",
      "STEP 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847fbb70c87a464f8c57d17984460fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "V step:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG entropy(not correct): 7.915862863714045\n",
      "train loss: 0.6128921939525758\n",
      "STEP 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8b051e077049c8baa59b0857f13ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "V step:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG entropy(not correct): 7.889609107723484\n",
      "train loss: 0.6746042658924123\n",
      "STEP 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40cd8f72b354862bb0a14ef471daa4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "V step:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG entropy(not correct): 7.860826043339519\n",
      "train loss: 0.7120309435995306\n",
      "STEP 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582ddabed16e4658a221aa68162e351f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikitext2 ppl (naive) 21.438142776489258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29721b4eeda4ef9b30ff0a5abe0e423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "V step:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG entropy(not correct): 7.829715837131847\n",
      "train loss: 0.7723719145869837\n",
      "STEP 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd8a99c3dc34e19a630e84912182558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "V step:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG entropy(not correct): 7.797919966957786\n",
      "train loss: 0.8470790250576108\n",
      "STEP 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8377e6893e4240dd91496de9f00919b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "V step:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG entropy(not correct): 7.764575208936419\n",
      "train loss: 0.9057074269512672\n",
      "STEP 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fcb1dd542474797af107bf02c9499c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "V step:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "last_ppl_eval_score = None\n",
    "for i in range(100):\n",
    "    print(\"STEP\", i)\n",
    "    if i % 5 == 0:\n",
    "        ppl_eval_score = eval_ppl_naive(dequantized_model, eval_data).item()\n",
    "        last_ppl_eval_score = ppl_eval_score\n",
    "        print(\"wikitext2 ppl (naive)\", ppl_eval_score)\n",
    "\n",
    "    v_step_train_loss = _run_one_step(args, base_model, dequantized_model, optimizer, next_train_data())\n",
    "    print(\"train loss:\", v_step_train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf53b3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
