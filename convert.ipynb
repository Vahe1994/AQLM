{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4808eb-c670-4aa9-bf5b-61772d377a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6764ff3a-6a1a-44a6-b069-d5c05e7dcf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import transformers\n",
    "\n",
    "MODEL_PATH = 'meta-llama/Llama-2-7b-hf'\n",
    "MODEL_SEQLEN = 4096\n",
    "\n",
    "\n",
    "@functools.cache\n",
    "def get_model():\n",
    "    return transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype='auto',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365ccc03-b6ba-411f-badf-e36a21aa7ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "\n",
    "from src.aq import QuantizedWeight, QuantizedLinear\n",
    "\n",
    "import torch\n",
    "import quiptools_cuda\n",
    "from matmul_had import get_hadK\n",
    "from fast_hadamard_transform import hadamard_transform\n",
    "\n",
    "torch.set_num_threads(16)\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "torch.backends.cuda.matmul.allow_tf32 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1dc4fd-ca4c-472a-ba27-d86591191a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_hadU_cuda(X, hadK, K):\n",
    "    n = X.shape[-1]\n",
    "    if K == 1:\n",
    "        return hadamard_transform(X.contiguous(), 1/(n**0.5))\n",
    "\n",
    "    input = X.float().view(-1, K, n // K)\n",
    "    input = hadamard_transform(input.contiguous(), 1/(n**0.5))\n",
    "    input = hadK.to(input.device).to(input.dtype) @ input\n",
    "    return input.to(X.device).to(X.dtype).reshape(X.shape)\n",
    "\n",
    "\n",
    "def vec_to_tuple(inp):\n",
    "    return tuple(v.item() for v in inp)\n",
    "\n",
    "\n",
    "class HadamardWrapper(nn.Module):\n",
    "    def __init__(self, SU, SV, inner, device='cuda'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.out_dim, self.in_dim = len(SV), len(SU)\n",
    "\n",
    "        SU = SU.detach().clone().to(device).float()\n",
    "        SU.requires_grad = True\n",
    "\n",
    "        SV = SV.detach().clone().to(device).float()\n",
    "        SV.requires_grad = True\n",
    "        \n",
    "        self.SU = torch.nn.Parameter(SU, requires_grad=True)\n",
    "        self.register_buffer(SV, requires_grad=True)\n",
    "        self.inner = inner\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_dim, in_dim = self.out_dim, self.in_dim\n",
    "        had_left_T, K_left = get_hadK(in_dim)\n",
    "        if had_left_T is not None:\n",
    "            had_left_T = had_left_T.T.contiguous()\n",
    "            assert had_left_T.requires_grad == False\n",
    "        had_right, K_right = get_hadK(out_dim)\n",
    "        if had_right is not None:\n",
    "            assert had_right.requires_grad == False\n",
    "        input_shape = x.shape\n",
    "        assert input_shape[-1] == in_dim\n",
    "        x = x.view(-1, in_dim)\n",
    "        # x = x.to(torch.float32)\n",
    "        x = x * self.SU\n",
    "        x = matmul_hadU_cuda(x, had_left_T, K_left) / 32\n",
    "        # x = x.to(torch.float16)\n",
    "        x = self.inner(x)\n",
    "        # x = x.to(torch.float32)\n",
    "        x = matmul_hadU_cuda(x, had_right, K_right)\n",
    "        x = x * self.SV * 32\n",
    "        # x = x.to(torch.float16)\n",
    "        x = x.reshape(tuple(input_shape[:-1]) + (out_dim,))\n",
    "        return x\n",
    "\n",
    "\n",
    "def replace_submodule(module, submodule_path, new_submodule):\n",
    "    submodule_names = submodule_path.split(\".\")\n",
    "    for submodule in submodule_names[:-1]:\n",
    "        module = getattr(module, submodule)\n",
    "    setattr(module, submodule_names[-1], new_submodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf785ec-d144-49ac-9dd6-a100dc3ea64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_tensor = torch.load('./e8p_grid.pt', map_location='cpu', weights_only=True)\n",
    "assert sorted([vec_to_tuple(x) for x in grid_tensor]) == [vec_to_tuple(x) for x in grid_tensor]\n",
    "\n",
    "grid_hashed = (grid_tensor * 4).round()\n",
    "assert grid_hashed.abs().max() < 127\n",
    "grid_hashed = grid_hashed.to(torch.int8).view(torch.int64).view(65536)\n",
    "idx_by_hash = {h.item(): idx for idx, h in enumerate(grid_hashed)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087c9653-2ccb-43e1-b5d1-e47e42b60ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grid\n",
    "\n",
    "packed_abs_grid = grid.get_packed_abs_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b323afe-3032-47ad-9b29-bf570bd9de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_codes(SU, SV, Qidxs):\n",
    "    in_dim, out_dim = len(SU), len(SV)\n",
    "    \n",
    "    W = quiptools_cuda.decompress_packed_e8p(\n",
    "        Qidxs.view(out_dim // 16, in_dim // 64, 8, 4).cuda(),\n",
    "        packed_abs_grid.cuda(),\n",
    "    ).cpu()\n",
    "    \n",
    "    W_hashed = W.cpu().reshape(out_dim * in_dim // 8, 8) * 4\n",
    "    assert W_hashed.abs().max() < 127\n",
    "    W_hashed = W_hashed.to(torch.int8).view(torch.int64).view(out_dim * in_dim // 8)\n",
    "    \n",
    "    W_codes = torch.tensor([\n",
    "        idx_by_hash[h.item()] for h in W_hashed\n",
    "    ]).reshape(out_dim, in_dim // 8)\n",
    "\n",
    "    return W_codes\n",
    "\n",
    "\n",
    "def get_quantized_weight(SU, SV, codes):  \n",
    "    in_dim, out_dim = len(SU), len(SV)\n",
    "    \n",
    "    quantized_weight = QuantizedWeight(\n",
    "        reference_weight=torch.ones((out_dim, in_dim), dtype=torch.float16).cuda(), num_codebooks=1,\n",
    "        nbits_per_codebook=16, scale_nbits=0, \n",
    "        out_group_size=1, in_group_size=8,\n",
    "        verbose=False, max_iter=0,\n",
    "    )\n",
    "    \n",
    "    quantized_weight.scales.data = torch.ones_like(quantized_weight.scales.data)\n",
    "    quantized_weight.scales.requires_grad = False\n",
    "    quantized_weight.codebooks.data = grid_tensor.reshape(quantized_weight.codebooks.shape).detach().clone().cuda()\n",
    "    quantized_weight.codes.data = codes.clone().reshape(quantized_weight.codes.shape).cuda().to(torch.int32)\n",
    "    return quantized_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4900815a-e09c-44cd-92ed-b30561a00207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "tensors = {}\n",
    "with safe_open(\"model.safetensors\", framework=\"pt\", device=\"cpu\") as f:\n",
    "   for key in f.keys():\n",
    "       tensors[key] = f.get_tensor(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df0c590-2098-433e-921c-4a86e0983367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quiptools_cuda\n",
    "\n",
    "def load_and_convert(layer):\n",
    "    Qidxs = tensors[f'{layer}.Qidxs']\n",
    "    SU = tensors[f'{layer}.SU'].float()\n",
    "    SV = tensors[f'{layer}.SV'].float() * tensors[f'{layer}.Wscale'].float()\n",
    "    fuse_scales = tensors.get(f'{layer}.fuse_scales', None)\n",
    "    if fuse_scales is not None:\n",
    "        fuse_scales = fuse_scales.float()\n",
    "    codebook_id = tensors[f'{layer}.codebook_id']\n",
    "    assert codebook_id.item() == 7\n",
    "    return SU, SV, get_codes(SU, SV, Qidxs), fuse_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dc3f87-9561-41f7-a0ad-f3a150894bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7d93f6-5d72-4685-9c50-6b475cceae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model().cuda()\n",
    "\n",
    "import copy\n",
    "\n",
    "model_changed = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661d035c-ef86-4a00-bad0-424026db1707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "for layer_idx in tqdm.tqdm(range(32)):\n",
    "    SU, SV, codes, fuse_scales = load_and_convert(f'model.layers.{layer_idx}.mlp.down_proj')\n",
    "    assert fuse_scales is None\n",
    "    down_linear = HadamardWrapper(SU, SV, QuantizedLinear(get_quantized_weight(SU, SV, codes), bias=None))\n",
    "    \n",
    "    # ---\n",
    "    \n",
    "    SU, SV, codes, fuse_scales = load_and_convert(f'model.layers.{layer_idx}.mlp.upgate_proj')\n",
    "    \n",
    "    up_out_dim = get_model().model.layers[layer_idx].mlp.up_proj.weight.shape[0]\n",
    "    gate_out_dim = get_model().model.layers[layer_idx].mlp.gate_proj.weight.shape[0]\n",
    "    \n",
    "    up_scale, gate_scale = fuse_scales\n",
    "    \n",
    "    scales = torch.cat([\n",
    "        up_scale * torch.ones((up_out_dim,), dtype=torch.float32),\n",
    "        gate_scale * torch.ones((gate_out_dim,), dtype=torch.float32),\n",
    "    ], dim=0)\n",
    "    SV = SV * scales\n",
    "    \n",
    "    upgate_linear = HadamardWrapper(SU, SV, QuantizedLinear(get_quantized_weight(SU, SV, codes), bias=None))\n",
    "    \n",
    "    # ---\n",
    "    \n",
    "    SU, SV, codes, fuse_scales = load_and_convert(f'model.layers.{layer_idx}.self_attn.o_proj')\n",
    "    assert fuse_scales is None\n",
    "    o_linear = HadamardWrapper(SU, SV, QuantizedLinear(get_quantized_weight(SU, SV, codes), bias=None))\n",
    "    \n",
    "    # ---\n",
    "    \n",
    "    SU, SV, codes, fuse_scales = load_and_convert(f'model.layers.{layer_idx}.self_attn.qkv_proj')\n",
    "    \n",
    "    q_out_dim = get_model().model.layers[layer_idx].self_attn.q_proj.weight.shape[0]\n",
    "    k_out_dim = get_model().model.layers[layer_idx].self_attn.k_proj.weight.shape[0]\n",
    "    v_out_dim = get_model().model.layers[layer_idx].self_attn.v_proj.weight.shape[0]\n",
    "    \n",
    "    q_scale, k_scale, v_scale = fuse_scales\n",
    "    \n",
    "    scales = torch.cat([\n",
    "        q_scale * torch.ones((q_out_dim,), dtype=torch.float32),\n",
    "        k_scale * torch.ones((k_out_dim,), dtype=torch.float32),\n",
    "        v_scale * torch.ones((v_out_dim,), dtype=torch.float32)\n",
    "    ], dim=0)\n",
    "    SV = SV * scales\n",
    "    \n",
    "    qkv_linear = HadamardWrapper(SU, SV, QuantizedLinear(get_quantized_weight(SU, SV, codes), bias=None))\n",
    "    \n",
    "    del model_changed.model.layers[layer_idx].self_attn.q_proj\n",
    "    del model_changed.model.layers[layer_idx].self_attn.k_proj\n",
    "    del model_changed.model.layers[layer_idx].self_attn.v_proj\n",
    "    del model_changed.model.layers[layer_idx].self_attn.o_proj\n",
    "    \n",
    "    model_changed.model.layers[layer_idx].self_attn.qkv_proj = qkv_linear\n",
    "    model_changed.model.layers[layer_idx].self_attn.o_proj = o_linear\n",
    "    \n",
    "    del model_changed.model.layers[layer_idx].mlp.gate_proj\n",
    "    del model_changed.model.layers[layer_idx].mlp.up_proj\n",
    "    del model_changed.model.layers[layer_idx].mlp.down_proj\n",
    "    \n",
    "    model_changed.model.layers[layer_idx].mlp.upgate_proj = upgate_linear\n",
    "    model_changed.model.layers[layer_idx].mlp.down_proj = down_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd92be-ebf4-4965-8e90-5c7fb9916ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_changed, 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690b1096-d48c-4ec2-96cd-a6d749d56e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./quip-sharp-model-aqlm-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f440ab4-a5b0-4837-8a42-b38d9b9445b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx, layer in enumerate(model_changed.model.layers):\n",
    "    torch.save(layer, f'./quip-sharp-model-aqlm-format/{layer_idx}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300bb420-2a18-479b-8674-a707fceebcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {k: v for k, v in model_changed.state_dict().items() if 'model.layers' not in k},\n",
    "    './quip-sharp-model-aqlm-format/not_quantized_weights.pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a8c534-fc06-44b0-a639-bcf8dfe812f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    dict(),\n",
    "    '/mnt/ar_home/galqiwi/tmp/quip-sharp-model/args.pt',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
